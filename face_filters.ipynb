{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab notebook\n",
    "import cv2\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the effects data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "crown       = cv2.imread('data/crown.png', cv2.IMREAD_UNCHANGED)\n",
    "glasses     = cv2.imread('data/glasses.png', cv2.IMREAD_UNCHANGED)\n",
    "flower      = cv2.imread('data/flower.png', cv2.IMREAD_UNCHANGED)\n",
    "eye_cartoon = cv2.imread('data/eye.png', cv2.IMREAD_UNCHANGED)\n",
    "b_hat       = cv2.imread('data/b_hat2.png', cv2.IMREAD_UNCHANGED)\n",
    "balloon     = cv2.imread('data/balloon.png', cv2.IMREAD_UNCHANGED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the Haar Cascades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "face_cascade  = cv2.CascadeClassifier(\"utils/haarcascade_frontalface_default.xml\")\n",
    "eye_cascade   = cv2.CascadeClassifier(\"utils/haarcascade_eye_tree_eyeglasses.xml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add the image categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_effects = 5\n",
    "# Function   : get_option_set\n",
    "# Input      : x - integer\n",
    "# Output     : a set of options representing the effect\n",
    "# Description: Return a set of effects to be performed\n",
    "def get_option_set(x):\n",
    "    return {\n",
    "        0: set([]),\n",
    "        1: set(['crown']),\n",
    "        2: set(['flower']),\n",
    "        3: set(['eye']),\n",
    "        4: set(['glasses']),\n",
    "        5: set(['crown', 'glasses'])\n",
    "    }[x]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Catch user input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function   : catch_ux_input\n",
    "# Input      : event - Event passed by asychronous call\n",
    "#              x     - x coordinate of event\n",
    "#              y     - y coordinate of event\n",
    "#              flags - Any special flags set\n",
    "#              param - User data if any\n",
    "# Output     : None\n",
    "# Description: Catch the asyncrhonous user input and fill the\n",
    "#              command queue based on the option entry parameter.\n",
    "def catch_ux_input(event,x,y,flags,param):\n",
    "    \n",
    "    global option_entry\n",
    "    if((x > 400) and (y > 440)):\n",
    "        if event == cv2.EVENT_LBUTTONDBLCLK:\n",
    "            option_entry = option_entry + 1\n",
    "            option_entry = option_entry % (num_effects + 1)\n",
    "            option = get_option_set(option_entry)\n",
    "            cmd_q.append(option)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alpha blending"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function   : alpha_blend\n",
    "# Input      : fg - foreground with alpha channel\n",
    "#              bg - background\n",
    "# Output     : blended image\n",
    "# Description: Blend the foreground on background\n",
    "# Note       : bg and fg must have same dimensions \n",
    "def alpha_blend(fg, bg):\n",
    "    b, g, r, a = cv2.split(fg)\n",
    "    mask = cv2.merge((a,a,a))\n",
    "    _, mask = cv2.threshold(mask,127,255,cv2.THRESH_BINARY)\n",
    "    mask = mask // 255\n",
    "    fg = cv2.cvtColor(fg, cv2.COLOR_BGRA2BGR)\n",
    "    fg = cv2.multiply(mask, fg)\n",
    "    bg = cv2.multiply(1-mask, bg)\n",
    "    return cv2.add(fg, bg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function   : set_effect\n",
    "# Input      : img    - video frame\n",
    "#              option - set of all effects to be performed\n",
    "# Output     : output frame\n",
    "# Description: Perform the requested effect on the video frame\n",
    "def set_effect(img, option):\n",
    "    \n",
    "    # Compute the gray scale version of frame\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Find all the faces in the frame\n",
    "    faces = face_cascade.detectMultiScale(gray, 1.3, 3)\n",
    "    \n",
    "    # For each face, apply the associated effects (if any)\n",
    "    for (x,y, w, h) in faces:\n",
    "        roi_g = gray[y:y+h, x:x+w]\n",
    "        roi_c = img[y:y+h, x:x+w]\n",
    "        \n",
    "        # Fetch the effect image as foreground\n",
    "        if('crown' in option or 'flower' in option or 'birthday' in option):\n",
    "            if('crown' in option):\n",
    "                fg = crown.copy()\n",
    "            elif('flower' in option):\n",
    "                fg = flower.copy()\n",
    "            elif('birthday' in option):\n",
    "                fg = b_hat.copy()\n",
    "                \n",
    "            # Scale the foreground wrt face size\n",
    "            sf = w / fg.shape[1]\n",
    "            dim = (int(round(fg.shape[1]*sf)), int(round(fg.shape[0]*sf)))\n",
    "            fg_rz = cv2.resize(fg, dim)\n",
    "            \n",
    "            # Extract the background frame to perform alpha blending\n",
    "            back = img[y-fg_rz.shape[0]:y, x:x+fg_rz.shape[1]]\n",
    "            img[y-fg_rz.shape[0]:y, x:x+fg_rz.shape[1]] = alpha_blend(fg_rz, back)\n",
    "\n",
    "        \n",
    "        # Fetch the combination of both eye effect images as foreground\n",
    "        if('glasses' in option or 'eye' in option):\n",
    "            if('glasses' in option):\n",
    "                fg = glasses.copy()\n",
    "            elif('eye' in option):\n",
    "                fg = eye_cartoon.copy()\n",
    "\n",
    "            # Find the eyes in the face region of interest\n",
    "            eye = eye_cascade.detectMultiScale(roi_g, 1.1, 3)\n",
    "            \n",
    "            # To avoid multiple false positives\n",
    "            if(len(eye) == 2):\n",
    "                \n",
    "                # Find the width in accordance to distance between the eye's end to end\n",
    "                w = abs(eye[0][0] - eye[1][0]) + max(eye[0][2], eye[1][2])\n",
    "                x = min(eye[0][0], eye[1][0])\n",
    "                y = max(eye[0][1], eye[1][1])\n",
    "                \n",
    "                # Scale the foreground according to requirement\n",
    "                sf = w / fg.shape[1]\n",
    "                dim = (int(round(fg.shape[1]*sf)), int(round(fg.shape[0]*sf)))\n",
    "                n_dim = (int(round(dim[0]*1.2)), int(round(dim[1]*1.2)))\n",
    "                hdiff_x = int(round((n_dim[0] - dim[0])/1.8))\n",
    "                hdiff_y = int(round((n_dim[1] - dim[1])/1.8))\n",
    "                \n",
    "                # Adjust some delta to accomodate more natural looking effects\n",
    "                y_pl = y - hdiff_y\n",
    "                x_pl = x - hdiff_x\n",
    "    \n",
    "                fg_rz = cv2.resize(fg, n_dim)\n",
    "        \n",
    "                # Get the background roi to perform alpha blending\n",
    "                back = roi_c[y_pl: y_pl+n_dim[1], x_pl:x_pl+n_dim[0]]\n",
    "                roi_c[y_pl: y_pl+n_dim[1], x_pl:x_pl+n_dim[0]] = alpha_blend(fg_rz, back)\n",
    "\n",
    "    return img            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start the video capture\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Press Enter to use camera or input file name\n",
      "As per user input: using webcam\n"
     ]
    }
   ],
   "source": [
    "# Use webcam or input file based on requirement\n",
    "filename = input(\"Press Enter to use camera or input file name\")\n",
    "if(filename == ''):\n",
    "    filename=0\n",
    "    print(\"As per user input: using webcam\")\n",
    "\n",
    "# Open the video capture stream\n",
    "test = cv2.VideoCapture(filename)\n",
    "\n",
    "# Set the option_entry and command queue as global to be used for mouse callback\n",
    "global option_entry, cmd_q\n",
    "option_entry = 0\n",
    "cmd_q = []\n",
    "\n",
    "# Set the mouse callback\n",
    "cv2.namedWindow('frame')\n",
    "cv2.setMouseCallback('frame',catch_ux_input)\n",
    "\n",
    "# For calculating FPS\n",
    "prev_time = time.time()\n",
    "num_frames = 0\n",
    "\n",
    "# Dummy option set\n",
    "option = set([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FPS:  13.74292868764855\n"
     ]
    }
   ],
   "source": [
    "while(True):\n",
    "    \n",
    "    num_frames = num_frames + 1\n",
    "    \n",
    "    # Read the input stream frame\n",
    "    ret, frame = test.read()\n",
    "    \n",
    "    # If there is command effect pending, do that\n",
    "    if(len(cmd_q) > 0):\n",
    "        option = cmd_q.pop(0)\n",
    "    \n",
    "    try:\n",
    "        \n",
    "        # Set the effect\n",
    "        out = set_effect(frame, option)\n",
    "        \n",
    "        # Print the option for user to provide input\n",
    "        font = cv2.FONT_HERSHEY_SCRIPT_COMPLEX\n",
    "        cv2.putText(out,'Change Effect',(400,465), font, 1.2,(127,127,255),2,cv2.LINE_AA)\n",
    "        \n",
    "        # Resize the OpenCV window for better visualization\n",
    "        out = cv2.resize(out, (1200, 900))\n",
    "        cv2.resizeWindow('frame', 1200,900)\n",
    "        \n",
    "        # Display the live effects to the user\n",
    "        cv2.imshow('frame',out)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # Continue looping till user presses 'q'\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Compute the frame rate.\n",
    "tic = time.time()\n",
    "print(\"FPS: \", num_frames/(tic - prev_time))\n",
    "\n",
    "# Release the capture object and destroy all open OpenCV windows\n",
    "test.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
